package test;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.TimeUnit;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.BlockLocation;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.InputFormat;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.JobContext;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;
import org.apache.hadoop.util.StopWatch;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;


public class Input extends InputFormat<LongWritable, Text> {

	public static final String INPUT_DIR = 
			"mapreduce.input.fileinputformat.inputdir";
	public static final String INPUT_FILE = 
			"mapreduce.input.selfinputformat.inputfile";
	public static final String SPLIT_MAXSIZE = 
			"mapreduce.input.fileinputformat.split.maxsize";
	public static final String SPLIT_MINSIZE = 
			"mapreduce.input.fileinputformat.split.minsize";
	public static final String PATHFILTER_CLASS = 
			"mapreduce.input.pathFilter.class";
	public static final String NUM_INPUT_FILES =
			"mapreduce.input.fileinputformat.numinputfiles";
	public static final String INPUT_DIR_RECURSIVE =
			"mapreduce.input.fileinputformat.input.dir.recursive";
	public static final String LIST_STATUS_NUM_THREADS =
			"mapreduce.input.fileinputformat.list-status.num-threads";
	public static final int DEFAULT_LIST_STATUS_NUM_THREADS = 1;
	
	private static final Log LOG = LogFactory.getLog(FileInputFormat.class);
	
	@Override
	public RecordReader<LongWritable, Text> createRecordReader(InputSplit arg0, TaskAttemptContext arg1)
			throws IOException, InterruptedException {
		//TextInputFormat使用的就是LineRecordReader，可以指定分隔符，默认是CR,LF,CRLF
		//返回结果的key是位置，value是这一行的内容
		//示例程序wc里面是按行读取，然后分成一个个的词统计
		return new LineRecordReader();
	}

	@Override
	public List<InputSplit> getSplits(JobContext job) throws IOException, InterruptedException {
	    //StopWatch sw = new StopWatch().start();
	    //long minSize = job.getConfiguration().getLong("mapreduce.input.fileinputformat.split.minsize", 1L);
	    //long maxSize = job.getConfiguration().getLong("mapreduce.input.fileinputformat.split.maxsize", Long.MAX_VALUE);

		StopWatch sw = new StopWatch().start();
		
		Configuration conf = job.getConfiguration();
		
		String infile = conf.get(INPUT_FILE, "");
		Path p = new Path(infile);
		
	    // generate splits
	    List<InputSplit> splits = new ArrayList<InputSplit>();
	    
	    FileSystem  fs = FileSystem.get(p.toUri(), conf);
	    FileStatus f = fs.getFileStatus(p);
	    
	    if (!f.isFile()) {
	    	throw new IOException(infile + " is not a file");
	    }
	    
	    FSDataInputStream ins = fs.open(f.getPath());
	    
	    
	    //public int read(long position, byte[] buffer, int offset, int length)
	    
	    byte[] buffer = new byte[1024 * 1024];
	    long splitpos = 0;
	    long readpos = 0;
	    int readed = 0;
	    int lineCnt = 0;
	    
	    
	    do {
	    	readed  = ins.read(readpos, buffer, 0, buffer.length);
	    	for (int offset = 0; offset < readed; ++offset) {
	    		if(buffer[offset] == '\n') {
		    		++lineCnt;
		    		if (lineCnt >= 1000000) {
		    			BlockLocation[] blk = fs.getFileBlockLocations(f, splitpos, readpos + offset + 1 - splitpos);
		    			splits.add(new FileSplit(p, splitpos, readpos + offset + 1 - splitpos, 
		    					blk[0].getHosts(), blk[0].getCachedHosts()));
		    			splitpos = readpos + offset + 1;
		    			//System.out.println("split file " + splitpos + "," + readpos + "," + readed + "," +lineCnt);
		    			lineCnt = 0;
		    		}
	    		}
	    	}
	    	readpos = readpos + readed;
	    }while(readed >= 0);
	    
	    BlockLocation[] blk = fs.getFileBlockLocations(f, splitpos, f.getLen() - splitpos);
	    splits.add(new FileSplit(p, splitpos, f.getLen() - splitpos, 
				blk[0].getHosts(), blk[0].getCachedHosts()));
	    
	    // Save the number of input files for metrics/loadgen
	    job.getConfiguration().setLong(NUM_INPUT_FILES, 1);
	    sw.stop();
	    if (LOG.isDebugEnabled()) {
	      LOG.debug("Total # of splits generated by getSplits: " + splits.size()
	          + ", TimeTaken: " + sw.now(TimeUnit.MILLISECONDS));
	    }
	    
	    return splits;
	    
	    
	}

}
